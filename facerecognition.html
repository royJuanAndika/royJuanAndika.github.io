<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developped by Orson.io team" name="author">

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">
  
  <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-icon-180x180.png">
  <link href="./assets/favicon.ico" rel="icon">
  
  <!-- Add Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <!-- Add syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

  <!-- Custom styles for code display -->
  <style>
    .code-container {
      background-color: #f5f5f5;
      border-radius: 5px;
      margin: 20px 0;
      overflow: hidden;
    }
    .code-tabs {
      display: flex;
      background-color: #333;
    }
    .code-tab {
      padding: 10px 15px;
      color: #ccc;
      cursor: pointer;
      border-bottom: 2px solid transparent;
    }
    .code-tab.active {
      background-color: #1e1e1e;
      color: white;
      border-bottom: 2px solid #4285f4;
    }
    .code-content {
      display: none;
      padding: 0;
      margin: 0;
    }
    .code-content.active {
      display: block;
    }
    pre {
      margin: 0;
      padding: 15px;
      background-color: #1e1e1e;
      border-radius: 0;
      overflow-x: auto;
    }
    code {
      font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
      font-size: 14px;
    }
    .system-overview {
      margin-bottom: 30px;
    }
    .tech-badge {
      display: inline-block;
      padding: 5px 10px;
      margin: 5px;
      background-color: #0078d7;
      color: white;
      border-radius: 20px;
      font-size: 12px;
    }
    
    /* Fix for large gap between content and footer */
    .section-container {
      padding-bottom: 20px !important; /* Override default padding */
    }
    
    /* Image Modal Styles */
    .modal {
      display: none;
      position: fixed;
      z-index: 1000;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.9);
      overflow: auto;
      transition: opacity 0.3s ease;
    }
    
    .modal-content {
      display: block;
      position: relative;
      margin: auto;
      max-width: 90%;
      max-height: 90vh;
      top: 50%;
      transform: translateY(-50%);
    }
    
    .close {
      position: absolute;
      top: 15px;
      right: 20px;
      color: #f1f1f1;
      font-size: 40px;
      font-weight: bold;
      transition: 0.3s;
      z-index: 1001;
    }
    
    .close:hover,
    .close:focus {
      color: #bbb;
      text-decoration: none;
      cursor: pointer;
    }
    
    .zoomable {
      cursor: pointer;
      transition: transform 0.2s;
    }
    
    .zoomable:hover {
      transform: scale(1.02);
    }
  </style>

  <title>Works - Face Recognition System</title>  

<link href="./main.3f6952e4.css" rel="stylesheet"></head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
  <nav class="navbar  navbar-fixed-top navbar-default">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav ">
          <li><a href="./index.html" title="">Home</a></li>
          <li><a href="./works.html" title="">Works</a></li>
          <li><a href="./about.html" title="">About me</a></li>
          <li><a href="./contact.html" title="">Contact</a></li>
          <!-- <li><a href="./components.html" title="">Components</a></li> -->
        </ul>


          <!-- <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
            <li><a href="./index.html" title="" class="active">001</a></li>
            <li><a href="./index.html" title="">002</a></li>
            <li><a href="./index.html" title="">003</a></li>
            <li><a href="./index.html" title="">004</a></li>
            <li><a href="./index.html" title="">005</a></li>
            <li><a href="./index.html" title="">006</a></li>
          </ul> -->


      </div> 
    </div>
  </nav>
</header>
<div class="section-container">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">002 : Face Recognition System</h1>
          </div>
        </div>
    
        <div class="container-fluid">
          
          <p>A high-accuracy face recognition system capable of detecting and recognizing faces in real-time using <span style="font-weight: bold;">SCRFD</span> for face detection and <span style="font-weight: bold;">ArcFace</span> for face recognition. This project was developed as part of the <span style="font-weight: bold;">Computer Vision Course Final Project</span>.</p>
          <h3>Overview</h3>
          <p>This face recognition system is designed to accurately detect and identify faces in images and video streams. The system employs two cutting-edge deep learning algorithms: <span style="font-weight: bold;">Sample and Computation Redistribution for Efficient Face Detection (SCRFD)</span> for face detection and <span style="font-weight: bold;">Additive Angular Margin Loss for Deep Face Recognition (ArcFace)</span> for face recognition.<br><br>
          
            The face detection component can locate faces in various lighting conditions, angles, and even when partially obscured by objects. The face recognition component converts detected faces into unique numerical vectors (embeddings) that can be compared to identify individuals. Using <span style="font-weight: bold;">cosine similarity</span>, the system matches these vectors against a database of known faces to determine identity.<br><br>
          
            Testing results demonstrate exceptional performance with a <span style="font-weight: bold;">100% accuracy rate</span> for face detection in controlled environments and high accuracy in face recognition tasks. The system only struggles with faces that are significantly turned away from the camera, substantially covered by other objects, or blurred due to movement. These limitations are due to the algorithm's reliance on key facial points around the eyes, nose, and mouth.<br><br>
            <div style="max-width: 50%; margin: 0 auto;">
              <img src="./assets/images/videoprocessing.png" alt="Face Matching Process" class="img-responsive zoomable">
            </div>
            The system processes each video frame sequentially, making it suitable for real-time applications. It maintains accuracy while being computationally efficient, making it deployable on standard hardware.
          </p>
          
          <div class="system-overview">
            <h3>Technology Stack</h3>
            <div>
              <span class="tech-badge">OpenCV</span>
              <span class="tech-badge">SCRFD</span>
              <span class="tech-badge">ArcFace</span>
              <span class="tech-badge">Python</span>
              <span class="tech-badge">NumPy</span>
              <span class="tech-badge">Cosine Similarity</span>
              <span class="tech-badge">CUDA</span>
            </div>
          </div>

          <div class="system-overview">
            <h3>Project Workflow</h3>
            <p>The face recognition system follows a structured workflow for processing images and video frames:</p>
            
            <h4>1. Face Detection with SCRFD</h4>
            <div class="col-xs-12">
              <div style="max-width: 50%; margin: 0 auto;">
              <img src="./assets/images/facedetection.png" alt="Face Detection Process" class="img-responsive zoomable">
              </div>
              <p>SCRFD employs a neural network approach to detect faces in images, identifying both the facial region (bounding box) and key facial landmarks. The model uses <strong>multi-scale feature detection</strong> to accurately locate faces of various sizes and angles. It processes images in three steps: generating anchor points across multiple scales (8, 16, and 32 pixels), detecting potential faces, and applying Non-Maximum Suppression to eliminate overlapping detections. This results in precise face locations even in challenging conditions.</p>
            </div>
            
            <h4>2. Face Recognition with ArcFace</h4>
            <p>ArcFace transforms detected face regions into unique 512-dimensional vector embeddings. Each face is represented as a unique numerical vector where similar faces have vectors with smaller angular distances between them. This embedding approach allows for efficient comparison and matching of faces across a database. The model is robust against variations in lighting, expression, and minor changes in appearance.</p>
            
            <h4>3. Face Matching with Cosine Similarity</h4>
            <div style="max-width: 50%; margin: 0 auto;">
              <img src="./assets/images/facematching.png" alt="Face Matching Process" class="img-responsive zoomable">
            </div>
            <p>After generating embeddings, the system matches unknown faces against known faces using cosine similarity:</p>
            <ul>
              <li><strong>Embedding Comparison</strong>: Each detected face's embedding is compared against all known faces in the database</li>
              <li><strong>Threshold-Based Recognition</strong>: A similarity score above 0.4 (configurable) is considered a match</li>
              <li><strong>Best Match Selection</strong>: The system selects the known face with the highest similarity score above the threshold</li>
            </ul>
            <p>This approach enables accurate recognition while minimizing false positives, even when processing multiple faces simultaneously.</p>
          </div>

          <div class="system-overview">
            <h3>System Architecture</h3>
            <div class="col-xs-12">
                <p>The system architecture implements a streamlined pipeline for efficient face processing:</p>
                
                <ol>
                    <li><span style="font-weight: bold;">Input Processing</span>: The system accepts images or video frames as input, supporting both real-time webcam feeds and pre-recorded videos</li>
                    <li><span style="font-weight: bold;">Face Detection</span>: SCRFD processes each frame to locate faces and their key points (eyes, nose, mouth corners)</li>
                    <li><span style="font-weight: bold;">Face Embedding</span>: ArcFace transforms each detected face into a 512-dimensional embedding vector</li>
                    <div style="max-width: 50%; margin: 0 auto;">
                      <img src="./assets/images/database.png" alt="Face Matching Process" class="img-responsive zoomable">
                    </div>
                    <li><span style="font-weight: bold;">Database Matching</span>: The system compares embeddings against a pre-built database of known faces</li>
                    <li><span style="font-weight: bold;">Result Visualization</span>: Recognized faces are annotated with bounding boxes and identification labels</li>
                </ol>
                
                <p>The face database is built during initialization by processing all images in the face dataset directory. Each face is detected, embedded, and stored with its associated identity for later comparison.</p>
                
                <p>This architecture achieves a balance between accuracy and performance, with key optimizations including:</p>
                
                <ul>
                    <li><span style="font-weight: bold;">Batch Processing</span>: Processing multiple faces in a single frame</li>
                    <li><span style="font-weight: bold;">Confidence Thresholds</span>: Configurable thresholds for detection and recognition accuracy</li>
                    <li><span style="font-weight: bold;">GPU Acceleration</span>: Support for CUDA acceleration on compatible hardware</li>
                </ul>
            </div>
          </div>

          <div class="system-overview">
            <h3>Testing Results</h3>
            <h4 class="text-center">Image Test Result</h4>
            <div class="col-xs-12">
              <p>The system was tested with a class group photo containing 32 faces and performed remarkably well:</p>
              <img src="./assets/images/32face.jpg" class="img-responsive zoomable" alt="Face Recognition Result">
              <p><span style="font-weight: bold;">Face Detection</span>: Successfully detected all 32 faces in the test image (100% detection rate), including partially obscured faces.</p> 
            </div>
            <div class="col-xs-12">
              <img src="./assets/images/32facezoomed.png" class="img-responsive zoomable" alt="Face Recognition Result Zoomed">
              <p><span style="font-weight: bold;">Face Recognition</span>: Correctly identified all 4 individuals whose face data was in the database without any false positives.</p>
            </div>
            <h4 class="text-center">Video Test Result</h4>
            <div class="col-xs-12">
              <p>Video demonstration of the system's performance in a classroom environment:</p>
              <p><a href="https://youtu.be/ca0DXwCXYrc" target="_blank" class="btn btn-primary">Watch System Demo Video</a></p>
              <div style="max-width: 50%; margin: 0 auto;">
                <img src="./assets/images/problems.png" alt="Face Matching Process" class="img-responsive zoomable">
              </div>
              <p>Testing revealed some limitations in face detection:</p>
              <ol>
                <li>Faces not oriented toward the camera</li>
                <li>Faces significantly obscured by objects (e.g., masks)</li>
                <li>Blurred faces due to movement</li>
              </ol>
              <p>These limitations are expected as the algorithm relies on detecting key facial points around the eyes, nose, and mouth.</p>
            </div>
          </div>

          <div class="system-overview">
            <h3>Key Implementation Code</h3>
            <p>The face recognition system is implemented with a modular architecture focusing on efficient processing and accurate recognition.</p>
          </div>

          <div class="code-container">
            <div class="code-tabs">
              <div class="code-tab active" onclick="changeTab(event, 'main')">Main Program</div>
              <div class="code-tab" onclick="changeTab(event, 'detector')">Face Detection</div>
              <div class="code-tab" onclick="changeTab(event, 'recognizer')">Face Recognition</div>
              <div class="code-tab" onclick="changeTab(event, 'frame-processor')">Face Matching (Frame Processing)</div>
              
            </div>
            
            <div id="main" class="code-content active">
              <pre><code class="language-python">
def main(params):
    setup_logging(params.log_level)

    # STEP 1: Instantiate face recognition and face detection models
    detector = SCRFD(params.det_weight, input_size=(640, 640), conf_thres=params.confidence_thresh)
    recognizer = ArcFace(params.rec_weight)

    # STEP 2: Build target face database from face dataset directory
    targets = build_targets(detector, recognizer, params)
    colors = {name: (random.randint(0, 256), random.randint(0, 256), random.randint(0, 256)) 
              for _, name in targets}

    cap = cv2.VideoCapture(params.source)
    if not cap.isOpened():
        raise Exception("Could not open video or webcam")
    
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    # STEP 3: Initialize video input from webcam
    out = cv2.VideoWriter("output_video.mp4", cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))
    
    # Create a resizable window
    cv2.namedWindow("Frame", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("Frame", width, height)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # STEP 4: Process each frame from the video input
        frame = frame_processor(frame, detector, recognizer, targets, colors, params)
        out.write(frame)
        cv2.imshow("Frame", frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()
              </code></pre>
            </div>
            
            <div id="detector" class="code-content">
              <pre><code class="language-python">
class SCRFD:
    def __init__(self, model_file=None, input_size=(640, 640), conf_thres=0.5):
        """
        Initialize SCRFD face detector
        
        Args:
            model_file: Path to ONNX model file
            input_size: Network input size (width, height)
            conf_thres: Confidence threshold for detections
        """
        self.net = cv2.dnn.readNet(model_file)
        self.input_size = input_size
        self.conf_threshold = conf_thres
        self.nms_threshold = 0.4
        
        # Enable GPU acceleration if available
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
        
        # Stride configuration for multi-scale detection
        self.strides = [8, 16, 32]
        self.fmc = len(self.strides)
        
    def detect(self, img, max_num=0):
        """
        Detect faces in an image
        
        Args:
            img: Input image
            max_num: Maximum number of faces to detect (0 for unlimited)
            
        Returns:
            bboxes: Array of bounding boxes with confidence scores
            kpss: Array of facial keypoints
        """
        # Preprocess image
        img_height, img_width = img.shape[:2]
        input_height, input_width = self.input_size
        
        blob = cv2.dnn.blobFromImage(
            img, 1.0/128, self.input_size, (127.5, 127.5, 127.5), swapRB=True
        )
        
        # Forward pass
        self.net.setInput(blob)
        outputs = self.net.forward(self.net.getUnconnectedOutLayersNames())
        
        # Process outputs
        scores_list = []
        bboxes_list = []
        kpss_list = []
        
        # Process each feature map (multi-scale outputs)
        for idx, stride in enumerate(self.strides):
            # Decode outputs to get face detections
            # Implementation details omitted for brevity
            
        # Apply non-maximum suppression
        return self._postprocess(scores_list, bboxes_list, kpss_list, 
                                img_height, img_width, max_num)
        
    def _postprocess(self, scores, bboxes, kpss, img_height, img_width, max_num):
        """
        Post-processing of detections
        """
        # Convert to numpy arrays
        scores = np.vstack(scores)
        bboxes = np.vstack(bboxes)
        kpss = np.vstack(kpss) if len(kpss) > 0 else np.zeros((0, 0, 2))
        
        # Apply NMS
        indices = cv2.dnn.NMSBoxes(
            bboxes, scores.flatten(), self.conf_threshold, self.nms_threshold
        )
        
        # Extract top detections
        # Implementation details omitted for brevity
        
        return final_bboxes, final_kpss
              </code></pre>
            </div>
            
            <div id="recognizer" class="code-content">
              <pre><code class="language-python">
class ArcFace:
    def __init__(self, model_file=None):
        """
        Initialize ArcFace face recognition model
        
        Args:
            model_file: Path to ONNX model file
        """
        self.model = cv2.dnn.readNet(model_file)
        
        # Enable GPU acceleration if available
        self.model.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
        self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
        
    def __call__(self, img, kps):
        """
        Generate face embedding
        
        Args:
            img: Input image
            kps: Facial keypoints (5 points: eyes, nose, mouth corners)
            
        Returns:
            embedding: Normalized face embedding vector
        """
        # Align face using keypoints
        aligned_face = self._align_face(img, kps)
        
        # Create blob from aligned face
        blob = cv2.dnn.blobFromImage(
            aligned_face, 1.0/128, (112, 112), (127.5, 127.5, 127.5), swapRB=True
        )
        
        # Forward pass
        self.model.setInput(blob)
        embedding = self.model.forward()
        
        # Normalize embedding (L2 normalization)
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding
        
    def _align_face(self, img, kps):
        """
        Align face using facial keypoints
        
        Args:
            img: Input image
            kps: Facial keypoints
            
        Returns:
            aligned_face: Aligned face image
        """
        # Define reference keypoints (destination)
        dst = np.array([
            [30.2946, 51.6963],  # left eye
            [65.5318, 51.6963],  # right eye
            [48.0252, 71.7366],  # nose
            [33.5493, 92.3655],  # left mouth
            [62.7299, 92.3655]   # right mouth
        ], dtype=np.float32)
        
        # Calculate transformation matrix
        M = cv2.estimateAffinePartial2D(kps, dst)[0]
        
        # Apply transformation
        aligned_face = cv2.warpAffine(img, M, (112, 112))
        
        return aligned_face

def compute_similarity(embedding1, embedding2):
    """
    Compute cosine similarity between two face embeddings
    
    Args:
        embedding1: First face embedding
        embedding2: Second face embedding
        
    Returns:
        similarity: Cosine similarity score (higher means more similar)
    """
    # Calculate dot product
    dot_product = np.dot(embedding1, embedding2)
    
    # Calculate magnitudes
    norm1 = np.linalg.norm(embedding1)
    norm2 = np.linalg.norm(embedding2)
    
    # Calculate cosine similarity
    similarity = dot_product / (norm1 * norm2)
    
    return similarity
              </code></pre>
            </div>
            
            <div id="frame-processor" class="code-content">
              <pre><code class="language-python">
def frame_processor(
    frame: np.ndarray,
    detector: SCRFD,
    recognizer: ArcFace,
    targets: List[Tuple[np.ndarray, str]],
    colors: dict,
    params: argparse.Namespace
) -> np.ndarray:
    """
    Process a single frame for face detection and recognition
    
    Args:
        frame: Input frame
        detector: SCRFD face detector
        recognizer: ArcFace face recognizer
        targets: List of (embedding, name) tuples for known faces
        colors: Dict mapping names to colors for visualization
        params: Configuration parameters
        
    Returns:
        processed_frame: Frame with annotated faces
    """
    # Detect faces in the frame
    bboxes, kpss = detector.detect(frame, params.max_num)

    # Process each detected face
    for bbox, kps in zip(bboxes, kpss):
        *bbox, conf_score = bbox.astype(np.int32)
        
        # Generate embedding for the detected face
        embedding = recognizer(frame, kps)

        # Find best match in targets
        max_similarity = 0
        best_match_name = "Unknown"
        
        for target, name in targets:
            similarity = compute_similarity(target, embedding)
            if similarity > max_similarity and similarity > params.similarity_thresh:
                max_similarity = similarity
                best_match_name = name

        # Draw bounding box with name and similarity score
        if best_match_name != "Unknown":
            color = colors[best_match_name]
            draw_bbox_info(frame, bbox, similarity=max_similarity, 
                          name=best_match_name, color=color)
        else:
            draw_bbox(frame, bbox, (0, 255, 0))

    return frame

def draw_bbox_info(img, bbox, similarity, name, color=(0, 255, 0)):
    """
    Draw bounding box with name and similarity score
    
    Args:
        img: Input image
        bbox: Bounding box coordinates [x1, y1, x2, y2]
        similarity: Similarity score
        name: Person name
        color: Box color
    """
    x1, y1, x2, y2 = bbox
    
    # Draw bounding box
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # Create label with name and similarity
    label = f"{name}: {similarity:.2f}"
    
    # Draw label background
    label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    cv2.rectangle(img, (x1, y1-label_size[1]-10), (x1+label_size[0], y1), color, -1)
    
    # Draw label text
    cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

def build_targets(detector, recognizer, params: argparse.Namespace) -> List[Tuple[np.ndarray, str]]:
    """
    Build target database from face images
    
    Args:
        detector: SCRFD face detector
        recognizer: ArcFace face recognizer
        params: Configuration parameters
        
    Returns:
        targets: List of (embedding, name) tuples
    """
    targets = []
    
    # Process each face image in the dataset directory
    for filename in os.listdir(params.faces_dir):
        name = filename[:-4]  # Remove file extension
        image_path = os.path.join(params.faces_dir, filename)

        # Read image
        image = cv2.imread(image_path)
        
        # Detect face
        bboxes, kpss = detector.detect(image, max_num=1)

        # Skip if no face detected
        if len(kpss) == 0:
            logging.warning(f"No face detected in {image_path}. Skipping...")
            continue

        # Generate embedding
        embedding = recognizer(image, kpss[0])
        targets.append((embedding, name))

    return targets
              </code></pre>
            </div>
          </div>

          <div class="system-overview">
            <h3>Project Conclusion</h3>
            <p>This face recognition system demonstrates the effective combination of state-of-the-art face detection and recognition technologies. Key achievements include:</p>
            
            <ul>
              <li><strong>High Accuracy</strong>: 100% detection rate for faces in test scenarios with reliable recognition</li>
              <li><strong>Real-time Processing</strong>: Efficient processing suitable for live video applications</li>
              <li><strong>Robust Recognition</strong>: Capable of identifying faces despite variations in lighting and minor obstructions</li>
            </ul>
            
            <p>Limitations primarily involve extreme face angles, heavy occlusion, and motion blur - challenges common to most contemporary face recognition systems.</p>
            
            <p>The project successfully implements a complete face recognition pipeline from detection to recognition using modern deep learning architectures (SCRFD and ArcFace) and demonstrates the practical application of computer vision techniques.</p>
          </div>
          <div class="system-overview">
            <h3>Complete Source Code</h3>
            <p>
            <div class="text-center"></div>
            <a href="https://drive.google.com/drive/folders/1UfsLmwIU3u3Qc9swOcsI7A6kESYV46qb?usp=sharing" class="btn btn-primary"
              style="margin-bottom: 15px; margin-right: 15px;">
              <i class="fa fa-file" style="margin-right: 8px;" aria-hidden="true"></i>
              Source Code (Google Drive)
            </a>
          </div>
          </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- Image Modal -->
<div id="imageModal" class="modal">
  <span class="close">&times;</span>
  <img class="modal-content" id="modalImage">
</div>

<footer class="footer-container text-center">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <p>Â© UNTITLED | Website created with <a href="http://www.mashup-template.com/" title="Create website with free html template">Mashup Template</a>/<a href="https://www.unsplash.com/" title="Beautiful Free Images">Unsplash</a></p>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
     
     // Set up image modal functionality
     const modal = document.getElementById("imageModal");
     const modalImg = document.getElementById("modalImage");
     const closeBtn = document.getElementsByClassName("close")[0];
     
     // Get all zoomable images
     const images = document.getElementsByClassName("zoomable");
     
     // Add click event to all zoomable images
     for (let i = 0; i < images.length; i++) {
       images[i].addEventListener("click", function() {
         modal.style.display = "block";
         modalImg.src = this.src;
       });
     }
     
     // Close the modal when clicking the close button
     closeBtn.addEventListener("click", function() {
       modal.style.display = "none";
     });
     
     // Close the modal when clicking outside the image
     modal.addEventListener("click", function(event) {
       if (event.target === modal) {
         modal.style.display = "none";
       }
     });
     
     // Close the modal with Escape key
     document.addEventListener("keydown", function(event) {
       if (event.key === "Escape" && modal.style.display === "block") {
         modal.style.display = "none";
       }
     });
  });
  
  function changeTab(event, tabId) {
    // Hide all content
    const contents = document.getElementsByClassName("code-content");
    for (let content of contents) {
      content.classList.remove("active");
    }
    
    // Deactivate all tabs
    const tabs = document.getElementsByClassName("code-tab");
    for (let tab of tabs) {
      tab.classList.remove("active");
    }
    
    // Activate clicked tab and its content
    document.getElementById(tabId).classList.add("active");
    event.currentTarget.classList.add("active");
  }
</script>

<script type="text/javascript" src="./main.70a66962.js"></script></body>

</html>